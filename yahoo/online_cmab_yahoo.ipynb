{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f8e7e-df31-474e-bf86-0b0c42d5b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f3497-5de1-4348-af2e-b48c709076a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yahoo.dataset as dataset\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4518e-400b-4017-97c9-2984c84e3b10",
   "metadata": {},
   "source": [
    "# Read Yahoo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d1c03-7d71-4dda-941c-d0565a438185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files = (\"../data/R6/ydata-fp-td-clicks-v1_0.20090502\",\"../data/R6/ydata-fp-td-clicks-v1_0.20090503\",\"../data/R6/ydata-fp-td-clicks-v1_0.20090509\")\n",
    "#dataset.get_yahoo_events(files)\n",
    "dataset.make_data_from_yahoo_events(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95ba27-2d33-451b-879f-0210be9cb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.X\n",
    "displays = dataset.displays\n",
    "rewards = dataset.rewards\n",
    "events = dataset.events\n",
    "article_feat = dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be2aad-6b64-434f-9cfc-a539eb78a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchoices = len(dataset.articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8343b-1bad-48d4-9f42-5be65c3d43c4",
   "metadata": {},
   "source": [
    "# Streaming update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db8574-9f00-4804-949c-88d041b94710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from contextualbandits.linreg import LinearRegression\n",
    "from contextualbandits.online import LinUCB, AdaptiveGreedy, \\\n",
    "        SoftmaxExplorer, ActiveExplorer, EpsilonGreedy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_sgd = SGDClassifier(random_state=123, loss='log', warm_start=False)\n",
    "base_ols = LinearRegression(lambda_=10., fit_intercept=True, method=\"sm\")\n",
    "\n",
    "## Metaheuristic using different base algorithms and configurations\n",
    "linucb = LinUCB(nchoices = nchoices, beta_prior = None, alpha = 0.1,\n",
    "                ucb_from_empty = False, random_state = 1111)\n",
    "### Important!!! the default hyperparameters for LinUCB in the reference paper\n",
    "### are very different from what's used in this example\n",
    "adaptive_active_greedy = AdaptiveGreedy(deepcopy(base_ols), nchoices = nchoices,\n",
    "                                        smoothing = None, beta_prior = ((3./nchoices,4.), 2),\n",
    "                                        active_choice = 'weighted', decay_type = 'percentile',\n",
    "                                        decay = 0.9997, batch_train = True,\n",
    "                                        random_state = 2222)\n",
    "\n",
    "\n",
    "linucb_both = LinUCB(nchoices = nchoices, beta_prior = None, alpha = 0.1,\n",
    "                ucb_from_empty = False, random_state = 1111)\n",
    "### Important!!! the default hyperparameters for LinUCB in the reference paper\n",
    "### are very different from what's used in this example\n",
    "adaptive_active_greedy_both = AdaptiveGreedy(deepcopy(base_ols), nchoices = nchoices,\n",
    "                                        smoothing = None, beta_prior = ((3./nchoices,4.), 2),\n",
    "                                        active_choice = 'weighted', decay_type = 'percentile',\n",
    "                                        decay = 0.9997, batch_train = True,\n",
    "                                        random_state = 2222)\n",
    "'''\n",
    "softmax_explorer = SoftmaxExplorer(deepcopy(base_sgd), nchoices = nchoices,\n",
    "                                   smoothing = (1,2), beta_prior = None, batch_train = True,\n",
    "                                   refit_buffer = 50, deep_copy_buffer = False, random_state = 3333)\n",
    "\n",
    "adaptive_greedy_perc = AdaptiveGreedy(deepcopy(base_ols), nchoices = nchoices,\n",
    "                                      smoothing = (1,2), beta_prior = None,\n",
    "                                      decay_type = 'percentile', decay = 0.9997, batch_train = True,\n",
    "                                      random_state = 4444)\n",
    "active_explorer = ActiveExplorer(deepcopy(base_sgd), smoothing = None, nchoices = nchoices,\n",
    "                                 beta_prior = ((3./nchoices, 4.), 2), batch_train = True, refit_buffer = 50,\n",
    "                                 deep_copy_buffer = False, random_state = 5555)\n",
    "epsilon_greedy_nodecay = EpsilonGreedy(deepcopy(base_ols), nchoices = nchoices,\n",
    "                                       smoothing = (1,2), beta_prior = None,\n",
    "                                       decay = None, batch_train = True,\n",
    "                                       deep_copy_buffer = False, random_state = 6666)\n",
    "'''\n",
    "\n",
    "models = [linucb, adaptive_active_greedy]\n",
    "          #, softmax_explorer\n",
    "          #, adaptive_greedy_perc,\n",
    "          #active_explorer, epsilon_greedy_nodecay]\n",
    "both_models = [linucb_both, adaptive_active_greedy_both]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94164c07-9a21-46fe-893b-96ac0285683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_lucb, rewards_aac = [list() for i in range(len(models))]\n",
    "\n",
    "lst_rewards = [rewards_lucb, rewards_aac]\n",
    "\n",
    "rewards_lucb_both, rewards_aac_both = [list() for i in range(len(both_models))]\n",
    "\n",
    "lst_rewards_both = [rewards_lucb_both, rewards_aac_both]\n",
    "\n",
    "# batch size - algorithms will be refit after N rounds\n",
    "batch_size=50\n",
    "\n",
    "    \n",
    "# these lists will keep track of which actions does each policy choose\n",
    "lst_a_lucb, lst_a_aac = [list() for i in range(len(models))]\n",
    "\n",
    "lst_actions = [lst_a_lucb, lst_a_aac]\n",
    "\n",
    "lst_a_lucb_both, lst_a_aac_both = [list() for i in range(len(both_models))]\n",
    "\n",
    "lst_actions_both = [lst_a_lucb_both, lst_a_aac_both]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567760f-cb76-4f4b-993f-3a2c37871551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_batch(events_batch, article_feat, batch_size):\n",
    "    cand_article_num_list = []\n",
    "    article_batch_add = []\n",
    "    for i in range(len(events_batch)):\n",
    "        cand_article_num_list.append(len(events_batch[i][1]))\n",
    "        article_batch_add.append(article_feat[events_batch[i][1]])\n",
    "    article_batch_add = np.concatenate(article_batch_add)\n",
    "    return(cand_article_num_list, article_batch_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724b6c1-65cd-4e91-b2d3-3829d1b13755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_value(actions_cand_this_batch,action_st,action_end,events_batch_ind):\n",
    "    action_value = []\n",
    "    row_ind = np.arange(action_st,action_end)\n",
    "    for i in range(len(row_ind)):\n",
    "        action_value.append(actions_cand_this_batch[row_ind[i],events_batch_ind[i]])\n",
    "    return(np.array(action_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35938fc5-f1a7-46cd-988f-4f571a60d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actions(actions_cand_this_batch,events_batch):\n",
    "    actions_this_batch = []\n",
    "    if actions_cand_this_batch.shape[0]==len(events_batch):\n",
    "        for i in range(len(events_batch)):\n",
    "            actions_this_batch.append(np.array(events_batch[i][1])[np.argmax(actions_cand_this_batch[i,events_batch[i][1]])])\n",
    "    else:\n",
    "        action_st = 0\n",
    "        for i in range(len(events_batch)):\n",
    "            action_end = action_st + len(events_batch[i][1])\n",
    "            action_value = extract_action_value(actions_cand_this_batch, action_st, action_end, events_batch[i][1])\n",
    "            actions_this_batch.append(np.array(events_batch[i][1])[np.argmax(action_value)])\n",
    "            action_st = action_end\n",
    "    actions_this_batch = np.array(actions_this_batch).astype('uint8')\n",
    "    return(actions_this_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc2252-2368-40ab-9268-5d7ac86805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actions_add(actions_cand_this_batch,events_batch):\n",
    "    actions_this_batch = []\n",
    "    action_st = 0\n",
    "    for i in range(len(events_batch)):\n",
    "        action_end = action_st + len(events_batch[i][1])\n",
    "        actions_this_batch.extend(list(np.array(events_batch[i][1])[np.argmax(actions_cand_this_batch[action_st:action_end,events_batch[i][1]],axis = 1)]))\n",
    "        action_st = action_end\n",
    "    actions_this_batch = np.array(actions_this_batch).astype('uint8')\n",
    "    return(actions_this_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39b385-13d0-47ae-a264-fa5fad49c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounds are simulated from the full dataset\n",
    "def simulate_rounds_stoch(model, rewards, actions_hist, X_batch, rewards_batch, display_batch, events_batch, rnd_seed):\n",
    "    np.random.seed(rnd_seed)\n",
    "    \n",
    "    ## choosing actions for this batch\n",
    "    actions_cand_this_batch = model.decision_function(X_batch)\n",
    "    actions_this_batch = extract_actions(actions_cand_this_batch,events_batch)\n",
    "    #actions_this_batch = model.predict(X_batch).astype('uint8')\n",
    "    \n",
    "    # rewards obtained now\n",
    "    rewards_batch[actions_this_batch!=display_batch] = 0\n",
    "    \n",
    "    # keeping track of the sum of rewards received\n",
    "    rewards.append(rewards_batch.sum())\n",
    "    \n",
    "    # adding this batch to the history of selected actions\n",
    "    new_actions_hist = np.append(actions_hist, actions_this_batch)\n",
    "    \n",
    "    # now refitting the algorithms after observing these new rewards\n",
    "    np.random.seed(rnd_seed)\n",
    "    model.partial_fit(X_batch, actions_this_batch, rewards_batch)\n",
    "    \n",
    "    return new_actions_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd74f15-f6b6-402a-a606-2c44290e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounds are simulated from the full dataset\n",
    "def simulate_rounds_stoch_bothcontext(model, rewards, actions_hist, X_batch_add, rewards_batch, rewards_batch_add, display_batch, events_batch, rnd_seed):\n",
    "    np.random.seed(rnd_seed)\n",
    "    \n",
    "    ## choosing actions for this batch\n",
    "    actions_cand_this_batch = model.decision_function(X_batch_add)\n",
    "    actions_this_batch = extract_actions(actions_cand_this_batch,events_batch)\n",
    "    actions_this_batch_add = extract_actions_add(actions_cand_this_batch,events_batch)\n",
    "    #actions_this_batch = model.predict(X_batch).astype('uint8')\n",
    "    \n",
    "    # rewards obtained now\n",
    "    rewards_batch[actions_this_batch!=display_batch] = 0\n",
    "    \n",
    "    # keeping track of the sum of rewards received\n",
    "    rewards.append(rewards_batch.sum())\n",
    "    \n",
    "    # adding this batch to the history of selected actions\n",
    "    new_actions_hist = np.append(actions_hist, actions_this_batch)\n",
    "    \n",
    "    # now refitting the algorithms after observing these new rewards\n",
    "    np.random.seed(rnd_seed)\n",
    "    model.partial_fit(X_batch_add, actions_this_batch_add, rewards_batch_add)\n",
    "    \n",
    "    return new_actions_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cae65-59e7-4db4-a1ac-34206d85327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now running all the simulation\n",
    "for i in range(int(np.floor(X.shape[0] / batch_size))):\n",
    "    batch_st = (i) * batch_size\n",
    "    batch_end = (i + 1) * batch_size\n",
    "    batch_end = np.min([batch_end, X.shape[0]])\n",
    "\n",
    "\n",
    "    X_batch = X[batch_st:batch_end, :]\n",
    "    display_batch = displays[batch_st:batch_end]\n",
    "    rewards_batch = np.array(rewards[batch_st:batch_end])\n",
    "    events_batch = events[batch_st:batch_end]\n",
    "\n",
    "    cand_article_num_list, article_batch_add = extract_event_batch(events_batch, article_feat, batch_size)\n",
    "\n",
    "\n",
    "    user_batch_add = np.repeat(X_batch, cand_article_num_list, axis=0)\n",
    "    display_batch_add = np.repeat(display_batch, cand_article_num_list, axis=0)\n",
    "    rewards_batch_add = np.repeat(rewards_batch, cand_article_num_list, axis=0)\n",
    "\n",
    "    X_batch_add = np.hstack((user_batch_add, article_batch_add))\n",
    "    \n",
    "    for model_idx in range(len(models)):\n",
    "        lst_actions[model_idx] = simulate_rounds_stoch(models[model_idx],\n",
    "                                                   lst_rewards[model_idx],\n",
    "                                                   lst_actions[model_idx],\n",
    "                                                   X_batch, rewards_batch, display_batch, events_batch,\n",
    "                                                   rnd_seed = batch_st)\n",
    "    for model_idx in range(len(both_models)):\n",
    "        lst_actions_both[model_idx] = simulate_rounds_stoch_bothcontext(both_models[model_idx],\n",
    "                                                   lst_rewards_both[model_idx],\n",
    "                                                   lst_actions_both[model_idx],\n",
    "                                                   X_batch_add, rewards_batch, rewards_batch_add, display_batch, events_batch,\n",
    "                                                   rnd_seed = batch_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43f102-a7ef-4088-912d-599d4d4d6b1c",
   "metadata": {},
   "source": [
    "# Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f562343-dd92-4b6f-98db-4dc63c5026a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(rewards_lucb),len(rewards_aac), len(rewards_lucb_both),len(rewards_aac_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9301d-c064-4597-ac9f-b25ad3544dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_lucb = rewards_lucb[:min_len]\n",
    "rewards_aac = rewards_aac[:min_len]\n",
    "rewards_lucb_both = rewards_lucb_both[:min_len]\n",
    "rewards_aac_both = rewards_aac_both[:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f30fd-06cb-479d-8af6-10fc1c4308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rewards_lucb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d4290-360a-4a16-b12e-8f41adb9c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_reward(reward_lst, batch_size=batch_size):\n",
    "    mean_rew=list()\n",
    "    for r in range(len(reward_lst)):\n",
    "        mean_rew.append(sum(reward_lst[:r+1]) * 1.0 / ((r+1)*batch_size))\n",
    "    return mean_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5deb9-6ae2-4851-9757-78cfc7786e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_lucb = get_mean_reward(rewards_lucb)\n",
    "mean_rewards_aac = get_mean_reward(rewards_aac)\n",
    "mean_reward_lucb_both = get_mean_reward(rewards_lucb_both)\n",
    "mean_rewards_aac_both = get_mean_reward(rewards_aac_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37470cd-c192-4b55-b8a7-ecd83d5a03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_mat = csr_matrix((rewards[:min_len], displays[:min_len], list(range(min_len))), shape=(min_len-1, nchoices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040dcf2-f476-41a9-b66d-319751ddeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "rcParams['figure.figsize'] = 25, 15\n",
    "lwd = 5\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors=plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "rcParams['figure.figsize'] = 25, 15\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(mean_reward_lucb, label=\"LinUCB (OLS)\", linewidth=lwd,color=colors[0])\n",
    "plt.plot(mean_rewards_aac, label=\"Adaptive Active Greedy (OLS)\", linewidth=lwd,color=colors[16])\n",
    "plt.plot(mean_reward_lucb_both, label=\"LinUCB with BothContext (OLS)\", linewidth=lwd,color=colors[12])\n",
    "plt.plot(mean_rewards_aac_both, label=\"Adaptive Active Greedy with BothContext (OLS)\", linewidth=lwd,color=colors[15])\n",
    "plt.plot(np.repeat(csr_mat.sum(0).max()/min_len,len(mean_reward_lucb)), label=\"Overall Best Arm (no context)\",linewidth=lwd,color=colors[1],ls='dashed')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 1.25])\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, ncol=3, prop={'size':20})\n",
    "\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=25)\n",
    "plt.xticks([i*20 for i in range(8)], [i*1000 for i in range(8)])\n",
    "\n",
    "\n",
    "plt.xlabel('Rounds (models were updated every 50 rounds)', size=30)\n",
    "plt.ylabel('Cumulative Mean Reward', size=30)\n",
    "plt.title('Comparison of Online Contextual Bandit Policies\\n(Streaming-data mode)\\n\\nYahoo Data',size=30)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f4a03-7d84-4f19-8d13-7b99bbb22382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0c7f2-0910-4a1a-ad06-bcd5e8bb8df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
